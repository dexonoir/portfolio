{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PH3406 Homework - Ng Chun Seng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.loadtxt(\"HW_Prob1_data.txt\")\n",
    "sigma_x = data[:,0]\n",
    "sigma_y = data[:,1]\n",
    "sigma_z = data[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct Inversion:\n",
    "* Bloch vector, $\\overrightarrow{v}_{DI}$ = $(\\langle\\sigma_x\\rangle,\\langle\\sigma_y\\rangle,\\langle\\sigma_z\\rangle)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_DI = [ 0.392 -0.336  0.876]\n",
      "||v_DI|| =  1.0168264355336165\n"
     ]
    }
   ],
   "source": [
    "average_sigma_x = np.sum(sigma_x)/len(sigma_x)\n",
    "average_sigma_y = np.sum(sigma_y)/len(sigma_y)\n",
    "average_sigma_z = np.sum(sigma_z)/len(sigma_z)\n",
    "v_DI = np.array([average_sigma_x,average_sigma_y,average_sigma_z])\n",
    "\n",
    "\n",
    "print(\"v_DI =\",v_DI)\n",
    "print(\"||v_DI|| = \", np.linalg.norm(v_DI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ||$\\overrightarrow{v}_{DI}$|| > 1, we require Maximum Likelihood Estimation\n",
    "* We use Kullback-Leibler divergence $\\mathcal{D}_{KL}(\\overrightarrow{v}_{DI},\\overrightarrow{v})$ = $N_{x+}ln\\left(\\frac{1+v_{DI,x}}{1+v_x}\\right)+ N_{x-}ln\\left(\\frac{1-v_{DI,x}}{1-v_x}\\right) + \n",
    "N_{y+}ln\\left(\\frac{1+v_{DI,y}}{1+v_y}\\right)+ N_{y-}ln\\left(\\frac{1-v_{DI,y}}{1-v_y}\\right) + N_{z+}ln\\left(\\frac{1+v_{DI,z}}{1+v_z}\\right)+ N_{z-}ln\\left(\\frac{1-v_{DI,z}}{1-v_z}\\right)$ \n",
    "\n",
    "* For which, we find a Bloch vector $\\overrightarrow{v}_{MLE}$ which minimizes $\\mathcal{D}_{KL}$, while ensuring ||$\\overrightarrow{v}_{DI}$|| $\\le 1$\n",
    "\n",
    "* $N_{m\\pm}$ = Number of states in the \"$\\pm$\" state on $\\overrightarrow{m}$ axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_MLE =  [ 0.37688405 -0.3165826   0.86934586]\n",
      "||v_DI|| =  0.9990136919527991\n"
     ]
    }
   ],
   "source": [
    "N_x_plus = len(sigma_x[sigma_x==1])\n",
    "N_x_minus = len(sigma_x[sigma_x==-1])\n",
    "N_y_plus = len(sigma_y[sigma_y==1])\n",
    "N_y_minus = len(sigma_y[sigma_y==-1])\n",
    "N_z_plus = len(sigma_z[sigma_z==1])\n",
    "N_z_minus = len(sigma_z[sigma_z==-1])\n",
    "\n",
    "def D_kl(v_x,v_y,v_z):\n",
    "    D_kl = (N_x_plus*np.log((1+average_sigma_x)/(1+v_x)) + N_x_minus*np.log((1-average_sigma_x)/(1-v_x)) + \n",
    "    N_y_plus*np.log((1+average_sigma_y)/(1+v_y)) + N_y_minus*np.log((1-average_sigma_y)/(1-v_y)) +\n",
    "    N_z_plus*np.log((1+average_sigma_z)/(1+v_z)) + N_z_minus*np.log((1-average_sigma_z)/(1-v_z))\n",
    "    )\n",
    "    return D_kl\n",
    "\n",
    "minimum_D_kl_value = 50         #set an arbitrary threshold value for the search of minimum value\n",
    "v_MLE = []\n",
    "for v_x in np.linspace(-0.999999,0.999999,200):         #look for the bloch vector v_MLE which minimizes D_KL while ensuring ||v_MLE||<=1 \n",
    "    for v_y in np.linspace(-0.999999,0.999999,200):\n",
    "        for v_z in np.linspace(-0.999999,0.999999,200):\n",
    "            if D_kl(v_x,v_y,v_z) > minimum_D_kl_value or np.linalg.norm(np.array([v_x,v_y,v_z])) > 1 :\n",
    "                continue\n",
    "            minimum_D_kl_value = D_kl(v_x,v_y,v_z)\n",
    "            v_MLE = np.array([v_x,v_y,v_z])\n",
    "\n",
    "print(\"v_MLE = \",v_MLE)\n",
    "print(\"||v_DI|| = \", np.linalg.norm(v_MLE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\hat{A}_m \\hat{\\rho}_j \\hat{A}_n^\\dagger = \\sum_k \\beta_{jk}^{mn}\\hat{\\rho}_k$ and we have\n",
    "* $\\hat{A}_0 = \\mathbb{I}$ ; $\\hat{A}_1 = \\mathbb{\\sigma_x}$ ; $\\hat{A}_2 = \\mathbb{\\sigma_y}$ ;  $\\hat{A}_3 = \\mathbb{\\sigma_z}$\n",
    "* $\\hat{\\rho}_k = |\\psi_k\\rangle\\langle\\psi_k|$, where $k\\in\\{0,1,2,3\\}$ form basis set for matrices space\n",
    "\n",
    "We determine complete matrix of $\\beta_{jk}^{mn}$ by first determining $\\beta_{jk}^{mn}$ for a particular $\\hat{\\rho}_k$ basis state before varying $\\hat{\\rho}_k$ and input states of interest $\\hat{\\rho}_j$ defined as per \n",
    "_Example 5.9_, i.e.: $|\\psi_k\\rangle\\langle\\psi_k|$ as we have \n",
    "* $\\hat{\\rho}'_k = \\mathcal{E}_1(|\\psi_k\\rangle\\langle\\psi_k|)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qutip import qeye, sigmax,sigmay,sigmaz\n",
    "\n",
    "def rho(k):\n",
    "    matrix = np.zeros(4)\n",
    "    matrix[k] = 1\n",
    "    return matrix.reshape((2,2))\n",
    "\n",
    "def A(m):\n",
    "    if m == 0:\n",
    "        return qeye(2).full()\n",
    "    if m == 1:\n",
    "        return sigmax().full()\n",
    "    if m == 2:\n",
    "        return sigmay().full()\n",
    "    if m == 3:\n",
    "        return sigmaz().full()\n",
    "    else: raise ValueError(\"Invalid operator\")\n",
    "    \n",
    "def beta(m,n, input):       #input = input state = rho_j     #beta = 2x2 matrix for each m,n value\n",
    "    return A(m) @ input @ A(n).conj().T\n",
    "\n",
    "def beta_mn_k(input,k):\n",
    "    beta_mn_row = np.zeros(16).tolist()            #compiling row elements of beta_mn via mn indexing for kth index of basis state\n",
    "    for m in range(4):\n",
    "        for n in range(4):\n",
    "            beta_mn = beta(m,n,input)\n",
    "            beta_mn_row[m*4+n] = beta_mn.reshape(-1)[k]\n",
    "    return beta_mn_row\n",
    "\n",
    "def full_beta_matrix():\n",
    "    full_matrix = []\n",
    "    for j in range(4):\n",
    "        for k in range(4):\n",
    "            full_matrix.append(beta_mn_k(rho(j),k))\n",
    "    return np.array(full_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we constructed $\\beta_{jk}^{mn}$, we find its inverse $(\\beta^{-1})_{jk}^{mn}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_inverse = np.linalg.inv(full_beta_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we act $(\\beta^{-1})_{jk}^{mn}$ on $\\lambda_{jk}$ vector, where _jk_ indices are compiled in similar indexing fashion as per _mn_ indices, as in prior sections\n",
    "\n",
    "We thus obtain $\\chi_{mn} =  \\sum_{jk}(\\beta^{-1})_{jk}^{mn} \\lambda_{jk}$\n",
    "\n",
    "* For the case of $\\gamma = 0$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0\n",
    "lambda_0 = np.array([[1,0],[0,0]]).reshape((2,2))\n",
    "lambda_1 = np.array([[0,np.sqrt(1-gamma)],[0,0]]).reshape((2,2))\n",
    "lambda_2 = np.array([[0,0],[np.sqrt(1-gamma),0]]).reshape((2,2))\n",
    "lambda_3 = np.array([[gamma,0],[0,1-gamma]]).reshape((2,2))\n",
    "\n",
    "lambda_vector = (lambda_0.reshape(-1)).tolist() + (lambda_1.reshape(-1)).tolist() + (lambda_2.reshape(-1)).tolist() + (lambda_3.reshape(-1)).tolist() \n",
    "\n",
    "chi = beta_inverse @ lambda_vector\n",
    "\n",
    "chi_matrix = chi.reshape((4,4))     #reshape to dimension of (m,n) = (4,4) as per d^2 x d^2\n",
    "\n",
    "print(chi_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For the case of $\\gamma = 0.15$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.92347722+0.j     0.        +0.j     0.        +0.j     0.0375    +0.j    ]\n",
      " [0.        +0.j     0.0375    +0.j     0.        -0.0375j 0.        +0.j    ]\n",
      " [0.        +0.j     0.        +0.0375j 0.0375    +0.j     0.        +0.j    ]\n",
      " [0.0375    +0.j     0.        +0.j     0.        +0.j     0.00152278+0.j    ]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.15\n",
    "lambda_0 = np.array([[1,0],[0,0]]).reshape((2,2))\n",
    "lambda_1 = np.array([[0,np.sqrt(1-gamma)],[0,0]]).reshape((2,2))\n",
    "lambda_2 = np.array([[0,0],[np.sqrt(1-gamma),0]]).reshape((2,2))\n",
    "lambda_3 = np.array([[gamma,0],[0,1-gamma]]).reshape((2,2))\n",
    "\n",
    "lambda_vector = (lambda_0.reshape(-1)).tolist() + (lambda_1.reshape(-1)).tolist() + (lambda_2.reshape(-1)).tolist() + (lambda_3.reshape(-1)).tolist() \n",
    "\n",
    "chi = beta_inverse @ lambda_vector\n",
    "\n",
    "chi_matrix = chi.reshape((4,4))     #reshape to dimension of (m,n) = (4,4) as per d^2 x d^2, for representation\n",
    "\n",
    "with np.printoptions(threshold=np.inf,linewidth=200):\n",
    "    print(chi_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
